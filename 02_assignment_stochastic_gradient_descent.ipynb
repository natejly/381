{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 2: Stochastic Gradient Descent and Momentum**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 02_assignment_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/02_assignment_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a linear function for the logistic regression task using the stochastic gradient descent and its momentum variant. We will test them on several binary classification datasets (breast cancer, digits larger or less than 5, and fir and pine coverage). We will implement a training and validation loop for the binary classification task and test it on the testing split for each dataset.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report training, validation, and testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean classification accuracies for all trained models across all datasets should be no more than 8% worse relative to the scores achieved by sci-kit learn's logistic regression model across training, validation and testing splits.\n",
        "\n",
        "- You may not use batch sizes of more than 10% of the training dataset size for stochastic gradient descent and momentum stochastic gradient descent.\n",
        "\n",
        "- You will only need to experiment with gradient descent (GD) and momentum gradient descent (momentum GD) on breast cancer and digits (toy) datasets. It will take too long to run them on fir and pine coverage (realistic) dataset to get reasonable numbers. Of course, you may try them on fir and pine coverage :) but they will not count towards your grade.\n",
        "\n",
        "- Note the run time speed up when comparing GD and momemtum GD with stochastic gradient descent (SGD) and momentum stochastic gradient descent (momentum SGD)! Even though they are faster and observing batches instead of the full dataset at each time step, they can still achieving similar accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LogisticRegression as LogisticRegressionSciKit\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of stochastic gradient descent optimizer for logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, alpha, eta_decay_factor, beta, optimizer_type):\n",
        "        '''\n",
        "        Arg(s):\n",
        "            alpha : float\n",
        "                initial learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "        '''\n",
        "\n",
        "        self.__alpha = alpha\n",
        "        self.__eta_decay_factor = eta_decay_factor\n",
        "        self.__beta = beta\n",
        "        self.__optimizer_type = optimizer_type\n",
        "        self.__momentum = None\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func='logistic'):\n",
        "        '''\n",
        "        Returns the gradient of a loss function\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement compute_gradient function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            N = x.shape[1]\n",
        "            z = y * (np.dot(w.T,x))\n",
        "            denom = 1 + np.exp(z)\n",
        "            num = y * x\n",
        "            grad = -np.sum(num / denom, axis=1) / N\n",
        "            return grad.reshape(-1, 1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def __polynomial_decay(self, time_step):\n",
        "        '''\n",
        "        Computes the polynomial decay factor t^{-a}\n",
        "\n",
        "        Arg(s):\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            float : polynomial decay to adjust (reduce) initial learning rate\n",
        "        '''\n",
        "        if self.__eta_decay_factor is None:\n",
        "            return 1.0\n",
        "        if self.__eta_decay_factor <= 0:\n",
        "            return 1.0\n",
        "        if time_step <= 0:\n",
        "            raise ValueError('Time step must be positive')\n",
        "        return time_step ** (-self.__eta_decay_factor)\n",
        "\n",
        "    def update(self,\n",
        "               w,\n",
        "               x,\n",
        "               y,\n",
        "               loss_func,\n",
        "               batch_size,\n",
        "               time_step):\n",
        "        '''\n",
        "        Updates the weight vector based on\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, should be 'logistic' for the purpose of the assignment\n",
        "            batch_size : int\n",
        "                batch size for stochastic and momentum stochastic gradient descent\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            numpy[float32]: d x 1 weights\n",
        "        '''\n",
        "        lr = self.__alpha * self.__polynomial_decay(time_step)\n",
        "\n",
        "        # TODO: Implement the optimizer update function\n",
        "        # For each optimizer type, compute gradients and update weights\n",
        "        grad = self.__compute_gradients(w, x, y, loss_func)\n",
        "        if self.__optimizer_type == 'gradient_descent':\n",
        "            # itterate\n",
        "            return w - lr * grad\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_gradient_descent':\n",
        "            # update momentum\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros_like(w)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1 - self.__beta) * grad\n",
        "            return w - lr * self.__momentum\n",
        "\n",
        "        elif self.__optimizer_type == 'stochastic_gradient_descent':\n",
        "            N = x.shape[1]\n",
        "            # shuffle data\n",
        "            indices = np.random.permutation(N)\n",
        "            x_shuffled = x.T[indices] \n",
        "            y_shuffled = y.T[indices]  \n",
        "            x_batch = x_shuffled[:batch_size].T \n",
        "            y_batch = y_shuffled[:batch_size].T  \n",
        "            grad = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            return w - lr * grad\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_stochastic_gradient_descent':\n",
        "            N = x.shape[1]\n",
        "            # shuffle data\n",
        "            indices = np.random.permutation(N)\n",
        "            x_shuffled = x.T[indices] \n",
        "            y_shuffled = y.T[indices]  \n",
        "            x_batch = x_shuffled[:batch_size].T \n",
        "            y_batch = y_shuffled[:batch_size].T  \n",
        "            grad = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros_like(w)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1 - self.__beta) * grad\n",
        "            return w - lr * self.__momentum\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported optimizer type: {}'.format(self.__optimizer_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRT2kC4GqAp_"
      },
      "source": [
        "Implementation of our logistic regression model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "vOaTyJ5VqBYt"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = None\n",
        "\n",
        "    def fit(self,\n",
        "            x,\n",
        "            y,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type,\n",
        "            loss_func='logistic'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            batch_size : int\n",
        "                number of examples per batch\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        '''\n",
        "\n",
        "        # TODO: Instantiate optimizer and weights\n",
        "        self.__optimizer = Optimizer(alpha, eta_decay_factor, beta, optimizer_type)\n",
        "\n",
        "        self.__weights = np.zeros((x.shape[0], 1), dtype=np.float32)\n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # TODO: Compute loss function\n",
        "            loss = self.__compute_loss(x, y, loss_func)\n",
        "            if (t % 100) == 0:\n",
        "                print('Step={}  Loss={}'.format(t, loss))\n",
        "\n",
        "            # TODO: Update weights\n",
        "            self.__weights = self.__optimizer.update(self.__weights, x, y, loss_func, batch_size, t)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the predict function\n",
        "        z = np.dot(self.__weights.T, x)  \n",
        "        return np.where(z > 0, 1, -1) \n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Computes the logistic loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the __compute_loss function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            z = np.dot(self.__weights.T, x)  # (1, N)\n",
        "            loss = np.mean(np.log(1 + np.exp(-y * z)))\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing logistic regression for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the breast cancer dataset (569 samples, 30 feature dimensions)\n",
            "***** Results on the breast cancer dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9413\n",
            "Validation set mean accuracy: 0.9649\n",
            "Testing set mean accuracy: 0.9737\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=1000\n",
            "Step=100  Loss=inf\n",
            "Step=200  Loss=inf\n",
            "Step=300  Loss=inf\n",
            "Step=400  Loss=inf\n",
            "Step=500  Loss=inf\n",
            "Step=600  Loss=inf\n",
            "Step=700  Loss=inf\n",
            "Step=800  Loss=inf\n",
            "Step=900  Loss=inf\n",
            "Step=1000  Loss=inf\n",
            "Total training time: 0.035987 seconds\n",
            "Training set mean accuracy: 0.9091\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9298\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=1000\n",
            "Step=100  Loss=14.716913021803\n",
            "Step=200  Loss=9.27293060796461\n",
            "Step=300  Loss=6.023980425426625\n",
            "Step=400  Loss=4.121167130533816\n",
            "Step=500  Loss=2.7090019101993734\n",
            "Step=600  Loss=7.087899853131206\n",
            "Step=700  Loss=3.951599933151608\n",
            "Step=800  Loss=1.7880394011852796\n",
            "Step=900  Loss=3.996432168056984\n",
            "Step=1000  Loss=1.875633525150613\n",
            "Total training time: 0.035237 seconds\n",
            "Training set mean accuracy: 0.9267\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9298\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=1 \n",
            "\t beta=None \n",
            "\t batch_size=32 \n",
            "\t T=2000\n",
            "Step=100  Loss=2.138300561908495\n",
            "Step=200  Loss=2.0578553016339125\n",
            "Step=300  Loss=2.0494191186388786\n",
            "Step=400  Loss=1.993681889675412\n",
            "Step=500  Loss=1.9653033658480914\n",
            "Step=600  Loss=1.9344651570860876\n",
            "Step=700  Loss=1.9289436399350128\n",
            "Step=800  Loss=1.918087053229877\n",
            "Step=900  Loss=1.9125290669688708\n",
            "Step=1000  Loss=1.8926277791072719\n",
            "Step=1100  Loss=1.8811666749005547\n",
            "Step=1200  Loss=1.8725948179087542\n",
            "Step=1300  Loss=1.874829719275801\n",
            "Step=1400  Loss=1.8597661316315435\n",
            "Step=1500  Loss=1.859160257289481\n",
            "Step=1600  Loss=1.8494436640462468\n",
            "Step=1700  Loss=1.8416214561843638\n",
            "Step=1800  Loss=1.8416898786827591\n",
            "Step=1900  Loss=1.835215749920749\n",
            "Step=2000  Loss=1.832222232363829\n",
            "Total training time: 0.115105 seconds\n",
            "Training set mean accuracy: 0.9003\n",
            "Validation set mean accuracy: 0.9123\n",
            "Testing set mean accuracy: 0.9123\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=1 \n",
            "\t beta=0.9 \n",
            "\t batch_size=32 \n",
            "\t T=2000\n",
            "Step=100  Loss=1.0678217194399429\n",
            "Step=200  Loss=1.0359148058515808\n",
            "Step=300  Loss=1.0492306784845624\n",
            "Step=400  Loss=0.9462301464832533\n",
            "Step=500  Loss=0.8873637364711011\n",
            "Step=600  Loss=0.866276524354255\n",
            "Step=700  Loss=0.8543013020902955\n",
            "Step=800  Loss=0.8409741903393992\n",
            "Step=900  Loss=0.8484891794027638\n",
            "Step=1000  Loss=0.8310383202966931\n",
            "Step=1100  Loss=0.8187004451127887\n",
            "Step=1200  Loss=0.814006018876023\n",
            "Step=1300  Loss=0.8114923760693743\n",
            "Step=1400  Loss=0.7984454645646836\n",
            "Step=1500  Loss=0.7952020894626726\n",
            "Step=1600  Loss=0.7902059299554696\n",
            "Step=1700  Loss=0.7864297169039033\n",
            "Step=1800  Loss=0.7825003510850133\n",
            "Step=1900  Loss=0.7767455429963576\n",
            "Step=2000  Loss=0.7754634636917677\n",
            "Total training time: 0.126454 seconds\n",
            "Training set mean accuracy: 0.9003\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9211\n",
            "\n",
            "Preprocessing the digits greater or less than 5 dataset (1797 samples, 64 feature dimensions)\n",
            "***** Results on the digits greater or less than 5 dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9184\n",
            "Validation set mean accuracy: 0.8802\n",
            "Testing set mean accuracy: 0.8944\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=2000\n",
            "Step=100  Loss=0.41372242673835435\n",
            "Step=200  Loss=0.3559152831120379\n",
            "Step=300  Loss=0.3292091547049941\n",
            "Step=400  Loss=0.31300168159452163\n",
            "Step=500  Loss=0.3017942580105165\n",
            "Step=600  Loss=0.2934414479441979\n",
            "Step=700  Loss=0.2869111810001685\n",
            "Step=800  Loss=0.281635533656002\n",
            "Step=900  Loss=0.27727106585337896\n",
            "Step=1000  Loss=0.2735950458506601\n",
            "Step=1100  Loss=0.2704550693478153\n",
            "Step=1200  Loss=0.2677423762173318\n",
            "Step=1300  Loss=0.2653767067700291\n",
            "Step=1400  Loss=0.2632972067377905\n",
            "Step=1500  Loss=0.2614567004082727\n",
            "Step=1600  Loss=0.25981793710695866\n",
            "Step=1700  Loss=0.2583510452999956\n",
            "Step=1800  Loss=0.2570317541732418\n",
            "Step=1900  Loss=0.25584011934867384\n",
            "Step=2000  Loss=0.25475958951537153\n",
            "Total training time: 0.341512 seconds\n",
            "Training set mean accuracy: 0.9109\n",
            "Validation set mean accuracy: 0.8886\n",
            "Testing set mean accuracy: 0.8972\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=2000\n",
            "Step=100  Loss=0.4150405566991785\n",
            "Step=200  Loss=0.35541452748887326\n",
            "Step=300  Loss=0.32875539515523533\n",
            "Step=400  Loss=0.31264870142237566\n",
            "Step=500  Loss=0.3015123742925512\n",
            "Step=600  Loss=0.2932069103101745\n",
            "Step=700  Loss=0.28670911366959384\n",
            "Step=800  Loss=0.2814568814812711\n",
            "Step=900  Loss=0.27711023675732155\n",
            "Step=1000  Loss=0.2734484766151104\n",
            "Step=1100  Loss=0.2703203925126674\n",
            "Step=1200  Loss=0.2676179394575969\n",
            "Step=1300  Loss=0.2652612943747107\n",
            "Step=1400  Loss=0.26318987688678885\n",
            "Step=1500  Loss=0.2613566891771144\n",
            "Step=1600  Loss=0.2597246011932801\n",
            "Step=1700  Loss=0.25826382736642917\n",
            "Step=1800  Loss=0.25695016147307054\n",
            "Step=1900  Loss=0.25576371019006044\n",
            "Step=2000  Loss=0.2546879643740237\n",
            "Total training time: 0.344298 seconds\n",
            "Training set mean accuracy: 0.9109\n",
            "Validation set mean accuracy: 0.8886\n",
            "Testing set mean accuracy: 0.8972\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=1 \n",
            "\t beta=None \n",
            "\t batch_size=64 \n",
            "\t T=3000\n",
            "Step=100  Loss=0.6538097291608793\n",
            "Step=200  Loss=0.6491615006109631\n",
            "Step=300  Loss=0.6465666971364341\n",
            "Step=400  Loss=0.6447245142781464\n",
            "Step=500  Loss=0.6432834247362804\n",
            "Step=600  Loss=0.642152590123711\n",
            "Step=700  Loss=0.6411754501389438\n",
            "Step=800  Loss=0.6403340192932188\n",
            "Step=900  Loss=0.6396026974814459\n",
            "Step=1000  Loss=0.6389518298651689\n",
            "Step=1100  Loss=0.6383637906628625\n",
            "Step=1200  Loss=0.6378194769026132\n",
            "Step=1300  Loss=0.6373318878458791\n",
            "Step=1400  Loss=0.6368801361034803\n",
            "Step=1500  Loss=0.6364548597032701\n",
            "Step=1600  Loss=0.6360670033590502\n",
            "Step=1700  Loss=0.6356885698044809\n",
            "Step=1800  Loss=0.6353450911255383\n",
            "Step=1900  Loss=0.6350080375275573\n",
            "Step=2000  Loss=0.6346926613386377\n",
            "Step=2100  Loss=0.6343995953831433\n",
            "Step=2200  Loss=0.6341184443872211\n",
            "Step=2300  Loss=0.6338508346275865\n",
            "Step=2400  Loss=0.6335924026136663\n",
            "Step=2500  Loss=0.6333386963028785\n",
            "Step=2600  Loss=0.6330965337240673\n",
            "Step=2700  Loss=0.6328722934483342\n",
            "Step=2800  Loss=0.6326507030563683\n",
            "Step=2900  Loss=0.6324347110691478\n",
            "Step=3000  Loss=0.6322313224581747\n",
            "Total training time: 0.678844 seconds\n",
            "Training set mean accuracy: 0.8219\n",
            "Validation set mean accuracy: 0.7660\n",
            "Testing set mean accuracy: 0.8167\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=1 \n",
            "\t beta=0.9 \n",
            "\t batch_size=64 \n",
            "\t T=3000\n",
            "Step=100  Loss=0.6711987250599178\n",
            "Step=200  Loss=0.6662063848715084\n",
            "Step=300  Loss=0.6633879255264761\n",
            "Step=400  Loss=0.6614158858417756\n",
            "Step=500  Loss=0.6598779358876904\n",
            "Step=600  Loss=0.6585926524993441\n",
            "Step=700  Loss=0.6574975445524719\n",
            "Step=800  Loss=0.6565780660345338\n",
            "Step=900  Loss=0.6557716313426682\n",
            "Step=1000  Loss=0.6550576130797713\n",
            "Step=1100  Loss=0.6543913650922386\n",
            "Step=1200  Loss=0.6538069126465695\n",
            "Step=1300  Loss=0.6532725648423748\n",
            "Step=1400  Loss=0.6527763645995\n",
            "Step=1500  Loss=0.6523004711458733\n",
            "Step=1600  Loss=0.6518695145744209\n",
            "Step=1700  Loss=0.651458163566795\n",
            "Step=1800  Loss=0.6510835913330694\n",
            "Step=1900  Loss=0.6507229316615861\n",
            "Step=2000  Loss=0.6503808347224268\n",
            "Step=2100  Loss=0.6500518925667702\n",
            "Step=2200  Loss=0.649751198971432\n",
            "Step=2300  Loss=0.6494556312759203\n",
            "Step=2400  Loss=0.6491709804914836\n",
            "Step=2500  Loss=0.6488991579138175\n",
            "Step=2600  Loss=0.6486400726884848\n",
            "Step=2700  Loss=0.648392681324749\n",
            "Step=2800  Loss=0.648154975160122\n",
            "Step=2900  Loss=0.6479222669360201\n",
            "Step=3000  Loss=0.647698740455203\n",
            "Total training time: 0.682816 seconds\n",
            "Training set mean accuracy: 0.8237\n",
            "Validation set mean accuracy: 0.7799\n",
            "Testing set mean accuracy: 0.8278\n",
            "\n",
            "Preprocessing the fir and pine coverage dataset (495141 samples, 54 feature dimensions)\n",
            "***** Results on the fir and pine coverage dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.7579\n",
            "Validation set mean accuracy: 0.7590\n",
            "Testing set mean accuracy: 0.7585\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=1 \n",
            "\t beta=None \n",
            "\t batch_size=256 \n",
            "\t T=5000\n",
            "Step=100  Loss=8.421760425129369\n",
            "Step=200  Loss=1.984163609023672\n",
            "Step=300  Loss=6.05194053018919\n",
            "Step=400  Loss=5.972417091957073\n",
            "Step=500  Loss=2.9742829923473133\n",
            "Step=600  Loss=3.1010090863654036\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[72], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# TODO: Train model on training set\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[43mmodel_ours\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_decay_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    200\u001b[0m time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{:3f}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time_elapsed))\n",
            "Cell \u001b[0;32mIn[71], line 59\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, x, y, T, alpha, eta_decay_factor, beta, batch_size, optimizer_type, loss_func)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m  Loss=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t, loss))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# TODO: Update weights\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[70], line 103\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[0;34m(self, w, x, y, loss_func, batch_size, time_step)\u001b[0m\n\u001b[1;32m     99\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__alpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__polynomial_decay(time_step)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# TODO: Implement the optimizer update function\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# For each optimizer type, compute gradients and update weights\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__compute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__optimizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_descent\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# itterate\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad\n",
            "Cell \u001b[0;32mIn[70], line 49\u001b[0m, in \u001b[0;36mOptimizer.__compute_gradients\u001b[0;34m(self, w, x, y, loss_func)\u001b[0m\n\u001b[1;32m     47\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(z)\n\u001b[1;32m     48\u001b[0m     num \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m x\n\u001b[0;32m---> 49\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m N\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2250\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2251\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2257\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load breast cancer, digits, and tree coverage datasets\n",
        "datasets = [\n",
        "    skdata.load_breast_cancer(),\n",
        "    skdata.load_digits(),\n",
        "    skdata.fetch_covtype()\n",
        "]\n",
        "dataset_names = [\n",
        "    'breast cancer',\n",
        "    'digits greater or less than 5',\n",
        "    'fir and pine coverage',\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_optimizer_types = [\n",
        "    # For breast cancer dataset\n",
        "    [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    ], [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For fir and pine coverage dataset\n",
        "    ], [\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# TODO: Select hyperparameters\n",
        "dataset_alphas = [\n",
        "    # For breast cancer dataset\n",
        "    [0.01, 0.01, 0.01, 0.01],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [0.001, 0.001, 0.001, 0.001],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.001, 0.001]\n",
        "]\n",
        "\n",
        "dataset_eta_decay_factors = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 1, 1],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 1, 1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [1, 1]\n",
        "]\n",
        "\n",
        "dataset_betas = [\n",
        "    # For breast cancer dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For fir and pine coverage dataset\n",
        "    [None, 0.9]\n",
        "]\n",
        "\n",
        "dataset_batch_sizes = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 32, 32],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 64, 64],\n",
        "    # For fir and pine coverage dataset\n",
        "    [256, 256]\n",
        "]\n",
        "\n",
        "dataset_Ts = [\n",
        "    # For breast cancer dataset\n",
        "    [1000, 1000, 2000, 2000],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [2000, 2000, 3000, 3000],\n",
        "    # For fir and pine coverage dataset\n",
        "    [5000, 5000]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_optimizer_types,\n",
        "    dataset_alphas,\n",
        "    dataset_eta_decay_factors,\n",
        "    dataset_betas,\n",
        "    dataset_batch_sizes,\n",
        "    dataset_Ts)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        optimizer_types, \\\n",
        "        alphas, \\\n",
        "        eta_decay_factors, \\\n",
        "        betas, \\\n",
        "        batch_sizes, \\\n",
        "        Ts = options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    if dataset_name == 'digits greater or less than 5':\n",
        "        y[y < 5] = 1\n",
        "        y[y >= 5] = 0\n",
        "    elif dataset_name == 'fir and pine coverage':\n",
        "\n",
        "        idx_fir_or_pine = np.where(np.logical_or(y == 1, y == 2))[0]\n",
        "\n",
        "        x = x[idx_fir_or_pine, :]\n",
        "        y = y[idx_fir_or_pine]\n",
        "\n",
        "        # Pine class: 0; Fir class: 1\n",
        "        y[y == 2] = 0\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, x.shape[0], x.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * x.shape[0])\n",
        "    val_split_idx = int(0.80 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests logistic regression model from scikit-learn\n",
        "    '''\n",
        "    model_scikit = LogisticRegressionSciKit(penalty=None, fit_intercept=False)\n",
        "\n",
        "    # TODO: Train scikit-learn logistic regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "    print('***** Results on the {} dataset using scikit-learn logistic regression model *****'.format(dataset_name))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "    score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
        "    print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "    score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
        "    print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "    score_test = skmetrics.accuracy_score(y_test, predictions_test)\n",
        "    print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    '''\n",
        "    Trains, validates, and tests our logistic regression model for binary classification\n",
        "    '''\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    # TODO: Set the ground truth to the appropriate classes (integers) according to lecture\n",
        "    # convert 0's to -1's\n",
        "    y_train = np.where(y_train == 0, -1, y_train)\n",
        "    y_val = np.where(y_val == 0, -1, y_val)\n",
        "    y_test = np.where(y_test == 0, -1, y_test)\n",
        "\n",
        "    model_options = zip(optimizer_types, alphas, eta_decay_factors, betas, batch_sizes, Ts)\n",
        "\n",
        "    for optimizer_type, alpha, eta_decay_factor, beta, batch_size, T in model_options:\n",
        "\n",
        "        # TODO: Initialize our logistic regression model\n",
        "        model_ours = LogisticRegression()\n",
        "\n",
        "        print('***** Results of our logistic regression model trained on {} dataset *****'.format(dataset_name))\n",
        "        print('\\t optimizer_type={} \\n\\t alpha={} \\n\\t eta_decay_factor={} \\n\\t beta={} \\n\\t batch_size={} \\n\\t T={}'.format(\n",
        "            optimizer_type, alpha, eta_decay_factor, beta, batch_size, T))\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        # TODO: Train model on training set\n",
        "        model_ours.fit(x_train, y_train, T, alpha, eta_decay_factor, beta, batch_size, optimizer_type)  \n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        print('Total training time: {:3f} seconds'.format(time_elapsed))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on training set\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "        score_train = skmetrics.accuracy_score(y_train.flatten(), predictions_train.flatten())\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on validation set\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "        score_val = skmetrics.accuracy_score(y_val.flatten(), predictions_val.flatten())\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on testing set\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "        score_test = skmetrics.accuracy_score(y_test.flatten(), predictions_test.flatten())\n",
        "        print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    print('')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
