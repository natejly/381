{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 2: Stochastic Gradient Descent and Momentum**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 02_assignment_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/02_assignment_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a linear function for the logistic regression task using the stochastic gradient descent and its momentum variant. We will test them on several binary classification datasets (breast cancer, digits larger or less than 5, and fir and pine coverage). We will implement a training and validation loop for the binary classification task and test it on the testing split for each dataset.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report training, validation, and testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean classification accuracies for all trained models across all datasets should be no more than 8% worse relative to the scores achieved by sci-kit learn's logistic regression model across training, validation and testing splits.\n",
        "\n",
        "- You may not use batch sizes of more than 10% of the training dataset size for stochastic gradient descent and momentum stochastic gradient descent.\n",
        "\n",
        "- You will only need to experiment with gradient descent (GD) and momentum gradient descent (momentum GD) on breast cancer and digits (toy) datasets. It will take too long to run them on fir and pine coverage (realistic) dataset to get reasonable numbers. Of course, you may try them on fir and pine coverage :) but they will not count towards your grade.\n",
        "\n",
        "- Note the run time speed up when comparing GD and momemtum GD with stochastic gradient descent (SGD) and momentum stochastic gradient descent (momentum SGD)! Even though they are faster and observing batches instead of the full dataset at each time step, they can still achieving similar accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LogisticRegression as LogisticRegressionSciKit\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of stochastic gradient descent optimizer for logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, alpha, eta_decay_factor, beta, optimizer_type):\n",
        "        '''\n",
        "        Arg(s):\n",
        "            alpha : float\n",
        "                initial learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "        '''\n",
        "\n",
        "        self.__alpha = alpha\n",
        "        self.__eta_decay_factor = eta_decay_factor\n",
        "        self.__beta = beta\n",
        "        self.__optimizer_type = optimizer_type\n",
        "        self.__momentum = None\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func='logistic'):\n",
        "        '''\n",
        "        Returns the gradient of a loss function\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement compute_gradient function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            N = x.shape[1]\n",
        "            z = y * (np.dot(w.T,x))\n",
        "            denom = 1 + np.exp(z)\n",
        "            num = y * x\n",
        "            grad = -np.sum(num / denom, axis=1) / N\n",
        "            return grad.reshape(-1, 1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def __polynomial_decay(self, time_step):\n",
        "        '''\n",
        "        Computes the polynomial decay factor t^{-a}\n",
        "\n",
        "        Arg(s):\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            float : polynomial decay to adjust (reduce) initial learning rate\n",
        "        '''\n",
        "        if self.__eta_decay_factor is None:\n",
        "            return 1.0\n",
        "        if self.__eta_decay_factor <= 0:\n",
        "            return 1.0\n",
        "        if time_step <= 0:\n",
        "            raise ValueError('Time step must be positive')\n",
        "        return time_step ** (-self.__eta_decay_factor)\n",
        "\n",
        "    def update(self,\n",
        "               w,\n",
        "               x,\n",
        "               y,\n",
        "               loss_func,\n",
        "               batch_size,\n",
        "               time_step):\n",
        "        '''\n",
        "        Updates the weight vector based on\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, should be 'logistic' for the purpose of the assignment\n",
        "            batch_size : int\n",
        "                batch size for stochastic and momentum stochastic gradient descent\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            numpy[float32]: d x 1 weights\n",
        "        '''\n",
        "        lr = self.__alpha * self.__polynomial_decay(time_step)\n",
        "\n",
        "        # TODO: Implement the optimizer update function\n",
        "        # For each optimizer type, compute gradients and update weights\n",
        "        grad = self.__compute_gradients(w, x, y, loss_func)\n",
        "        if self.__optimizer_type == 'gradient_descent':\n",
        "            # itterate\n",
        "            return w - lr * grad\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_gradient_descent':\n",
        "            # update momentum\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros_like(w)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1 - self.__beta) * grad\n",
        "            return w - lr * self.__momentum\n",
        "\n",
        "        elif self.__optimizer_type == 'stochastic_gradient_descent':\n",
        "            N = x.shape[1]\n",
        "            # shuffle data\n",
        "            indices = np.random.permutation(N)\n",
        "            x_shuffled = x.T[indices] \n",
        "            y_shuffled = y.T[indices]  \n",
        "            x_batch = x_shuffled[:batch_size].T \n",
        "            y_batch = y_shuffled[:batch_size].T  \n",
        "            grad = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            return w - lr * grad\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_stochastic_gradient_descent':\n",
        "            N = x.shape[1]\n",
        "            # shuffle data\n",
        "            indices = np.random.permutation(N)\n",
        "            x_shuffled = x.T[indices] \n",
        "            y_shuffled = y.T[indices]  \n",
        "            x_batch = x_shuffled[:batch_size].T \n",
        "            y_batch = y_shuffled[:batch_size].T  \n",
        "            grad = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros_like(w)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1 - self.__beta) * grad\n",
        "            return w - lr * self.__momentum\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported optimizer type: {}'.format(self.__optimizer_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRT2kC4GqAp_"
      },
      "source": [
        "Implementation of our logistic regression model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vOaTyJ5VqBYt"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = None\n",
        "\n",
        "    def fit(self,\n",
        "            x,\n",
        "            y,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type,\n",
        "            loss_func='logistic'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            batch_size : int\n",
        "                number of examples per batch\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        '''\n",
        "\n",
        "        # TODO: Instantiate optimizer and weights\n",
        "        self.__optimizer = Optimizer(alpha, eta_decay_factor, beta, optimizer_type)\n",
        "\n",
        "        self.__weights = np.zeros((x.shape[0], 1), dtype=np.float32)\n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # TODO: Compute loss function\n",
        "            loss = self.__compute_loss(x, y, loss_func)\n",
        "            if (t % 10000) == 0:\n",
        "                print('Step={}  Loss={}'.format(t, loss))\n",
        "\n",
        "            # TODO: Update weights\n",
        "            self.__weights = self.__optimizer.update(self.__weights, x, y, loss_func, batch_size, t)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the predict function\n",
        "        z = np.dot(self.__weights.T, x)  \n",
        "        sig = 1 / (1 + np.exp(-z))  \n",
        "        return np.where(sig >= 0.5, 1, -1) \n",
        "    \n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Computes the logistic loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the __compute_loss function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            z = np.dot(self.__weights.T, x)  \n",
        "            loss = np.mean(np.log(1 + np.exp(-y * z)))\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing logistic regression for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the breast cancer dataset (569 samples, 30 feature dimensions)\n",
            "***** Results on the breast cancer dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9355\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9298\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=10000  Loss=0.2903085142409005\n",
            "Total training time: 0.335429 seconds\n",
            "Training set mean accuracy: 0.9208\n",
            "Validation set mean accuracy: 0.9298\n",
            "Testing set mean accuracy: 0.9035\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=10000  Loss=0.17278983605962947\n",
            "Total training time: 0.432598 seconds\n",
            "Training set mean accuracy: 0.9238\n",
            "Validation set mean accuracy: 0.9386\n",
            "Testing set mean accuracy: 0.9035\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=0.5 \n",
            "\t beta=None \n",
            "\t batch_size=34 \n",
            "\t T=200000\n",
            "Step=10000  Loss=0.22985778842168397\n",
            "Step=20000  Loss=0.21970266949342426\n",
            "Step=30000  Loss=0.2149852310350894\n",
            "Step=40000  Loss=0.21291256691550983\n",
            "Step=50000  Loss=0.2104380884088789\n",
            "Step=60000  Loss=0.20909242008499165\n",
            "Step=70000  Loss=0.2078529830929266\n",
            "Step=80000  Loss=0.20692956163007586\n",
            "Step=90000  Loss=0.20639198308356183\n",
            "Step=100000  Loss=0.20575036467596977\n",
            "Step=110000  Loss=0.20498062738607115\n",
            "Step=120000  Loss=0.20441586196438125\n",
            "Step=130000  Loss=0.2040471111532782\n",
            "Step=140000  Loss=0.20368842748325203\n",
            "Step=150000  Loss=0.20329260182301903\n",
            "Step=160000  Loss=0.2027611263460274\n",
            "Step=170000  Loss=0.2024368610439995\n",
            "Step=180000  Loss=0.20211503096782915\n",
            "Step=190000  Loss=0.2018237293075803\n",
            "Step=200000  Loss=0.2018742238815235\n",
            "Total training time: 11.478329 seconds\n",
            "Training set mean accuracy: 0.9179\n",
            "Validation set mean accuracy: 0.9298\n",
            "Testing set mean accuracy: 0.9035\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=0.5 \n",
            "\t beta=0.9 \n",
            "\t batch_size=34 \n",
            "\t T=200000\n",
            "Step=10000  Loss=0.2316590852262425\n",
            "Step=20000  Loss=0.22210054271895394\n",
            "Step=30000  Loss=0.2154737449312446\n",
            "Step=40000  Loss=0.21262425271792776\n",
            "Step=50000  Loss=0.21069140755947424\n",
            "Step=60000  Loss=0.20928839292245793\n",
            "Step=70000  Loss=0.2080740752623165\n",
            "Step=80000  Loss=0.20732062854591785\n",
            "Step=90000  Loss=0.20630164459250916\n",
            "Step=100000  Loss=0.20559591798263913\n",
            "Step=110000  Loss=0.20504384661485878\n",
            "Step=120000  Loss=0.20447110138894187\n",
            "Step=130000  Loss=0.20421179019210978\n",
            "Step=140000  Loss=0.20381593881350601\n",
            "Step=150000  Loss=0.2034087243301398\n",
            "Step=160000  Loss=0.20280588497403093\n",
            "Step=170000  Loss=0.20247183844488323\n",
            "Step=180000  Loss=0.20224258177479543\n",
            "Step=190000  Loss=0.20187267948699736\n",
            "Step=200000  Loss=0.20159556646471627\n",
            "Total training time: 11.532451 seconds\n",
            "Training set mean accuracy: 0.9208\n",
            "Validation set mean accuracy: 0.9298\n",
            "Testing set mean accuracy: 0.9035\n",
            "\n",
            "Preprocessing the digits greater or less than 5 dataset (1797 samples, 64 feature dimensions)\n",
            "***** Results on the digits greater or less than 5 dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9035\n",
            "Validation set mean accuracy: 0.8663\n",
            "Testing set mean accuracy: 0.9000\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.005 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=20000\n",
            "Step=10000  Loss=0.244387531417709\n",
            "Step=20000  Loss=0.24277320961101692\n",
            "Total training time: 2.710289 seconds\n",
            "Training set mean accuracy: 0.8998\n",
            "Validation set mean accuracy: 0.8691\n",
            "Testing set mean accuracy: 0.9028\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.005 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=20000\n",
            "Step=10000  Loss=0.2443871966884419\n",
            "Step=20000  Loss=0.242772922813918\n",
            "Total training time: 2.714349 seconds\n",
            "Training set mean accuracy: 0.8998\n",
            "Validation set mean accuracy: 0.8691\n",
            "Testing set mean accuracy: 0.9028\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.5 \n",
            "\t beta=None \n",
            "\t batch_size=107 \n",
            "\t T=30000\n",
            "Step=10000  Loss=0.261813193599257\n",
            "Step=20000  Loss=0.2564011237287529\n",
            "Step=30000  Loss=0.2541253538344087\n",
            "Total training time: 5.674910 seconds\n",
            "Training set mean accuracy: 0.8998\n",
            "Validation set mean accuracy: 0.8747\n",
            "Testing set mean accuracy: 0.9167\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.5 \n",
            "\t beta=0.9 \n",
            "\t batch_size=107 \n",
            "\t T=30000\n",
            "Step=10000  Loss=0.26232996825333815\n",
            "Step=20000  Loss=0.25661812806594037\n",
            "Step=30000  Loss=0.25429252355498544\n",
            "Total training time: 6.029009 seconds\n",
            "Training set mean accuracy: 0.9007\n",
            "Validation set mean accuracy: 0.8774\n",
            "Testing set mean accuracy: 0.9139\n",
            "\n",
            "Preprocessing the fir and pine coverage dataset (495141 samples, 54 feature dimensions)\n",
            "***** Results on the fir and pine coverage dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.7432\n",
            "Validation set mean accuracy: 0.7447\n",
            "Testing set mean accuracy: 0.7433\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=0.95 \n",
            "\t beta=None \n",
            "\t batch_size=1743 \n",
            "\t T=5000\n",
            "Total training time: 455.674244 seconds\n",
            "Training set mean accuracy: 0.7169\n",
            "Validation set mean accuracy: 0.7179\n",
            "Testing set mean accuracy: 0.7182\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=0.95 \n",
            "\t beta=0.95 \n",
            "\t batch_size=1743 \n",
            "\t T=5000\n",
            "Total training time: 449.982237 seconds\n",
            "Training set mean accuracy: 0.7126\n",
            "Validation set mean accuracy: 0.7149\n",
            "Testing set mean accuracy: 0.7143\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load breast cancer, digits, and tree coverage datasets\n",
        "datasets = [\n",
        "    skdata.load_breast_cancer(),\n",
        "    skdata.load_digits(),\n",
        "    skdata.fetch_covtype()\n",
        "]\n",
        "dataset_names = [\n",
        "    'breast cancer',\n",
        "    'digits greater or less than 5',\n",
        "    'fir and pine coverage',\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_optimizer_types = [\n",
        "    # For breast cancer dataset\n",
        "    [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    ], [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For fir and pine coverage dataset\n",
        "    ], [\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# TODO: Select hyperparameters\n",
        "dataset_alphas = [\n",
        "    # For breast cancer dataset\n",
        "    [1e-4, 1e-4, 1e-4, 1e-4],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [5e-3, 5e-3, 1e-2, 1e-2],\n",
        "    # For fir and pine coverage dataset\n",
        "    [1e-3, 1e-3]\n",
        "]\n",
        "\n",
        "dataset_eta_decay_factors = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 0.5, 0.5],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 0.5, 0.5],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.95, 0.95]\n",
        "]\n",
        "\n",
        "dataset_betas = [\n",
        "    # For breast cancer dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For fir and pine coverage dataset\n",
        "    [None, 0.95]\n",
        "]\n",
        "\n",
        "cancer_batch = int(datasets[0].data.shape[0] * 0.6 * 0.1)\n",
        "digits_batch = int(datasets[1].data.shape[0] * 0.6 * 0.1)\n",
        "tree_batch = int(datasets[2].data.shape[0] * 0.6 * 0.005)\n",
        "dataset_batch_sizes = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, cancer_batch, cancer_batch],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, digits_batch, digits_batch],\n",
        "    # For fir and pine coverage dataset\n",
        "    [tree_batch, tree_batch]\n",
        "]\n",
        "\n",
        "dataset_Ts = [\n",
        "    # For breast cancer dataset\n",
        "    [10000, 10000, 200000, 200000],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [20000, 20000, 30000, 30000],\n",
        "    # For fir and pine coverage dataset\n",
        "    [5000, 5000]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_optimizer_types,\n",
        "    dataset_alphas,\n",
        "    dataset_eta_decay_factors,\n",
        "    dataset_betas,\n",
        "    dataset_batch_sizes,\n",
        "    dataset_Ts)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        optimizer_types, \\\n",
        "        alphas, \\\n",
        "        eta_decay_factors, \\\n",
        "        betas, \\\n",
        "        batch_sizes, \\\n",
        "        Ts = options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    if dataset_name == 'digits greater or less than 5':\n",
        "        y[y < 5] = 1\n",
        "        y[y >= 5] = 0\n",
        "    elif dataset_name == 'fir and pine coverage':\n",
        "\n",
        "        idx_fir_or_pine = np.where(np.logical_or(y == 1, y == 2))[0]\n",
        "\n",
        "        x = x[idx_fir_or_pine, :]\n",
        "        y = y[idx_fir_or_pine]\n",
        "\n",
        "        # Pine class: 0; Fir class: 1\n",
        "        y[y == 2] = 0\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, x.shape[0], x.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * x.shape[0])\n",
        "    val_split_idx = int(0.80 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests logistic regression model from scikit-learn\n",
        "    '''\n",
        "    model_scikit = LogisticRegressionSciKit(penalty=None, fit_intercept=False)\n",
        "\n",
        "    # TODO: Train scikit-learn logistic regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "    print('***** Results on the {} dataset using scikit-learn logistic regression model *****'.format(dataset_name))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "    score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
        "    print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "    score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
        "    print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "    # TODO: Score model using mean accuracy on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "    score_test = skmetrics.accuracy_score(y_test, predictions_test)\n",
        "    print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    '''\n",
        "    Trains, validates, and tests our logistic regression model for binary classification\n",
        "    '''\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    # TODO: Set the ground truth to the appropriate classes (integers) according to lecture\n",
        "    # convert 0's to -1's\n",
        "    y_train = np.where(y_train == 0, -1, y_train)\n",
        "    y_val = np.where(y_val == 0, -1, y_val)\n",
        "    y_test = np.where(y_test == 0, -1, y_test)\n",
        "\n",
        "    model_options = zip(optimizer_types, alphas, eta_decay_factors, betas, batch_sizes, Ts)\n",
        "\n",
        "    for optimizer_type, alpha, eta_decay_factor, beta, batch_size, T in model_options:\n",
        "        # skip gradient descent and momentum gradient descent for fir and pine coverage dataset\n",
        "        if dataset_name == 'fir and pine coverage' and optimizer_type in ['gradient_descent', 'momentum_gradient_descent']:\n",
        "            continue\n",
        "        # TODO: Initialize our logistic regression model\n",
        "        model_ours = LogisticRegression()\n",
        "\n",
        "        print('***** Results of our logistic regression model trained on {} dataset *****'.format(dataset_name))\n",
        "        print('\\t optimizer_type={} \\n\\t alpha={} \\n\\t eta_decay_factor={} \\n\\t beta={} \\n\\t batch_size={} \\n\\t T={}'.format(\n",
        "            optimizer_type, alpha, eta_decay_factor, beta, batch_size, T))\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        # TODO: Train model on training set\n",
        "        model_ours.fit(x_train, y_train, T, alpha, eta_decay_factor, beta, batch_size, optimizer_type)  \n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        print('Total training time: {:3f} seconds'.format(time_elapsed))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on training set\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "        score_train = skmetrics.accuracy_score(y_train.flatten(), predictions_train.flatten())\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on validation set\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "        score_val = skmetrics.accuracy_score(y_val.flatten(), predictions_val.flatten())\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # TODO: Score model using mean accuracy on testing set\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "        score_test = skmetrics.accuracy_score(y_test.flatten(), predictions_test.flatten())\n",
        "        print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    print('')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
