{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fsGaVMMpwG"
      },
      "source": [
        "**Exercise 4: Gradient Descent for Linear Regression**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 04_exercise_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/04_exercise_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this exercise, we will optimize a linear function for the regression task using the gradient descent for mean squared and half mean squared losses. We will test them on several datasets.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report validation and testing scores here.\n",
        "\n",
        "For full credit, your mean squared error scores for models optimized using mean_squared and half_mean_squared losses on Diabetes dataset should be no more than 15% worse the mean squared error scores achieved by sci-kit learn's linear regression model across training, validation and testing splits. Your mean squared error scores on California housing price dataset should be no more than 20% worse.\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxeZsiCGC0J8"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uumvcyiQ-k21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSciKit\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljMielQC7Lg"
      },
      "source": [
        "Implementation of our Gradient Descent optimizer for mean squared and half mean squared loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6wlWiioqDBkG"
      },
      "outputs": [],
      "source": [
        "class GradientDescentOptimizer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func):\n",
        "        '''\n",
        "        Returns the gradient of mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss type either mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the __compute_gradients function\n",
        "        if loss_func == 'mean_squared':\n",
        "            # TODO: Implements gradients for mean squared loss\n",
        "\n",
        "            '''\n",
        "            Using for-loop\n",
        "\n",
        "            gradients = np.zeros(x.shape)\n",
        "\n",
        "            for n in range((x.shape[1])):\n",
        "                x_n = x[:, n]\n",
        "                gradients[:, n] = (np.matmul(w.T, x_n) - y[n]) * x_n\n",
        "            '''\n",
        "\n",
        "            # Using matrix multiplication\n",
        "            gradients = (np.matmul(w.T, x) - y) * x\n",
        "\n",
        "            # Note: Set keepdims=True to keep the dimension of 1 (otherwise it will get squashed by mean operation)\n",
        "            return 2.0 * np.mean(gradients, axis=1, keepdims=True)\n",
        "        elif loss_func == 'half_mean_squared':\n",
        "            # TODO: Implements gradients for half mean squared loss\n",
        "\n",
        "            gradients = (np.matmul(w.T, x) - y) * x\n",
        "\n",
        "            return np.mean(gradients, axis=1, keepdims=True)\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def update(self, w, x, y, alpha, loss_func):\n",
        "        '''\n",
        "        Updates the weight vector based on mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            loss_func : str\n",
        "                loss type either 'mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 weights\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement the optimizer update function\n",
        "\n",
        "        return w - alpha * self.__compute_gradients(w, x, y, loss_func)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xOsR-kJIlD3"
      },
      "source": [
        "Implementation of Linear Regression with Gradient Descent optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BqpoUg5fIlgZ"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGradientDescent(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = GradientDescentOptimizer()\n",
        "\n",
        "    def fit(self, x, y, T, alpha, loss_func='mean_squared'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            loss_func : str\n",
        "                loss function to use\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement the fit function\n",
        "        self.__weights = np.zeros([x.shape[0], 1])\n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # TODO: Compute loss function\n",
        "            loss = self._compute_loss(\n",
        "                x=x,\n",
        "                y=y,\n",
        "                loss_func=loss_func)\n",
        "\n",
        "            if (t % 10000) == 0:\n",
        "                print('Step={}  Loss={:.4f}'.format(t, loss))\n",
        "\n",
        "            # TODO: Update weights\n",
        "            self.__weights = self.__optimizer.update(\n",
        "                w=self.__weights,\n",
        "                x=x,\n",
        "                y=y,\n",
        "                alpha=alpha,\n",
        "                loss_func=loss_func)\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the predict function\n",
        "\n",
        "        return np.matmul(self.__weights.T, x)\n",
        "\n",
        "    def _compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Returns the gradient of the mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss type either 'mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # TODO: Implements the _compute_loss function\n",
        "        predictions = self.predict(x)\n",
        "\n",
        "        if loss_func == 'mean_squared':\n",
        "            # TODO: Implements loss for mean squared loss\n",
        "            loss = np.mean((predictions - y) ** 2)\n",
        "        elif loss_func == 'half_mean_squared':\n",
        "            # TODO: Implements loss for half mean squared loss\n",
        "            loss = 0.50 * np.mean((predictions - y) ** 2)\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcb2TArNKvf1"
      },
      "source": [
        "Implementing training and validation loop for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB29ajtrK8sQ",
        "outputId": "0581fdd4-72a8-4a02-b3a2-b7e7843bc9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Results of scikit-learn linear regression model on Diabetes dataset *****\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Validation set mean squared error: 0.0000\n",
            "Validation set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "***** Results of our linear regression model trained with mean_squared loss, alpha=1e-06 and T=100000 on Diabetes dataset *****\n",
            "Step=10000  Loss=29451.2412\n",
            "Step=20000  Loss=29450.4203\n",
            "Step=30000  Loss=29449.5996\n",
            "Step=40000  Loss=29448.7792\n",
            "Step=50000  Loss=29447.9591\n",
            "Step=60000  Loss=29447.1393\n",
            "Step=70000  Loss=29446.3197\n",
            "Step=80000  Loss=29445.5004\n",
            "Step=90000  Loss=29444.6813\n",
            "Step=100000  Loss=29443.8625\n",
            "Training set mean squared error: 29443.8625\n",
            "Training set r-squared scores: 0.0000\n",
            "Validation set mean squared error: 0.0000\n",
            "Validation set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "***** Results of our linear regression model trained with half_mean_squared loss, alpha=2e-06 and T=100000 on Diabetes dataset *****\n",
            "Step=10000  Loss=14725.6206\n",
            "Step=20000  Loss=14725.2101\n",
            "Step=30000  Loss=14724.7998\n",
            "Step=40000  Loss=14724.3896\n",
            "Step=50000  Loss=14723.9796\n",
            "Step=60000  Loss=14723.5696\n",
            "Step=70000  Loss=14723.1598\n",
            "Step=80000  Loss=14722.7502\n",
            "Step=90000  Loss=14722.3407\n",
            "Step=100000  Loss=14721.9313\n",
            "Training set mean squared error: 29443.8625\n",
            "Training set r-squared scores: 0.0000\n",
            "Validation set mean squared error: 0.0000\n",
            "Validation set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "***** Results of scikit-learn linear regression model on California housing prices dataset *****\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Validation set mean squared error: 0.0000\n",
            "Validation set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "***** Results of our linear regression model trained with mean_squared loss, alpha=1e-06 and T=100000 on California housing prices dataset *****\n",
            "Step=10000  Loss=nan\n",
            "Step=20000  Loss=nan\n",
            "Step=30000  Loss=nan\n",
            "Step=40000  Loss=nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 116\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m***** Results of our linear regression model trained with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss, alpha=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and T=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dataset *****\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    113\u001b[0m     loss_func, alpha, T, dataset_name))\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# TODO: Train model on training set\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43mmodel_ours\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# TODO: Make pedictions\u001b[39;00m\n\u001b[1;32m    124\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m model_ours\u001b[38;5;241m.\u001b[39mpredict(x_train)\n",
            "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mLinearRegressionGradientDescent.fit\u001b[0;34m(self, x, y, T, alpha, loss_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# TODO: Compute loss function\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__compute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m  Loss=\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t, loss))\n",
            "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mLinearRegressionGradientDescent.__compute_loss\u001b[0;34m(self, x, y, loss_func)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mReturns the gradient of the mean squared or half mean squared loss\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    float : loss\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# TODO: Implements the __compute_loss function\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# TODO: Implements loss for mean squared loss\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((predictions \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
            "Cell \u001b[0;32mIn[3], line 61\u001b[0m, in \u001b[0;36mLinearRegressionGradientDescent.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mPredicts the label for each feature vector x\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    numpy[float32] : 1 x N vector\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# TODO: Implements the predict function\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load Diabetes and California housing prices dataset\n",
        "datasets = [\n",
        "    skdata.load_diabetes(),\n",
        "    skdata.fetch_california_housing()\n",
        "]\n",
        "dataset_names = [\n",
        "    'Diabetes',\n",
        "    'California housing prices'\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_loss_funcs = [\n",
        "    ['mean_squared', 'half_mean_squared'],\n",
        "    ['mean_squared', 'half_mean_squared']\n",
        "]\n",
        "\n",
        "# TODO: Select learning rates (alpha) for mean squared and half mean squared loss\n",
        "dataset_alphas = [\n",
        "    [1e-6, 2e-6],\n",
        "    [1e-6, 2e-6]\n",
        "]\n",
        "\n",
        "# TODO: Select number of steps (T) to train for mean squared and half mean squared loss\n",
        "dataset_Ts = [\n",
        "    [100000, 100000],\n",
        "    [100000, 100000]\n",
        "]\n",
        "\n",
        "for dataset_options in zip(datasets, dataset_names, dataset_loss_funcs, dataset_alphas, dataset_Ts):\n",
        "\n",
        "    dataset, dataset_name, loss_funcs, alphas, Ts = dataset_options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 80% as training set, next 10% as validation and the rest as testing\n",
        "    train_split_idx = int(0.80 * x.shape[0])\n",
        "    val_split_idx = int(0.90 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests Linear Regression model from scikit-learn\n",
        "    '''\n",
        "    # TODO: Initialize scikit-learn linear regression model without bias\n",
        "    model_scikit = None\n",
        "\n",
        "    # TODO: Trains scikit-learn linear regression model\n",
        "\n",
        "\n",
        "    print('***** Results of scikit-learn linear regression model on {} dataset *****'.format(\n",
        "        dataset_name))\n",
        "\n",
        "    # TODO: Test model on training set\n",
        "    predictions_train = None\n",
        "\n",
        "    score_mse_train = 0.0\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_train))\n",
        "\n",
        "    score_r2_train = 0.0\n",
        "    print('Training set r-squared scores: {:.4f}'.format(score_r2_train))\n",
        "\n",
        "    # TODO: Test model on validation set\n",
        "    predictions_val = None\n",
        "\n",
        "    score_mse_val = 0.0\n",
        "    print('Validation set mean squared error: {:.4f}'.format(score_mse_val))\n",
        "\n",
        "    score_r2_val = 0.0\n",
        "    print('Validation set r-squared scores: {:.4f}'.format(score_r2_val))\n",
        "\n",
        "    # TODO: Test model on testing set\n",
        "    predictions_test = None\n",
        "\n",
        "    score_mse_test = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_test))\n",
        "\n",
        "    score_r2_test = 0.0\n",
        "    print('Testing set r-squared scores: {:.4f}'.format(score_r2_test))\n",
        "\n",
        "    '''\n",
        "    Trains and tests our linear regression model using different solvers\n",
        "    '''\n",
        "\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    for loss_func, alpha, T in zip(loss_funcs, alphas, Ts):\n",
        "\n",
        "        # TODO: Initialize our linear regression model\n",
        "        model_ours = LinearRegressionGradientDescent()\n",
        "\n",
        "        print('***** Results of our linear regression model trained with {} loss, alpha={} and T={} on {} dataset *****'.format(\n",
        "            loss_func, alpha, T, dataset_name))\n",
        "\n",
        "        # TODO: Train model on training set\n",
        "        model_ours.fit(\n",
        "            x=x_train,\n",
        "            y=y_train,\n",
        "            T=T,\n",
        "            alpha=alpha,\n",
        "            loss_func=loss_func)\n",
        "\n",
        "        # TODO: Make pedictions\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "\n",
        "        # TODO: Test model on training set using mean squared error and r-squared\n",
        "        score_mse_train = skmetrics.mean_squared_error(y_train, predictions_train)\n",
        "        print('Training set mean squared error: {:.4f}'.format(score_mse_train))\n",
        "\n",
        "        score_r2_train = 0.0\n",
        "        print('Training set r-squared scores: {:.4f}'.format(score_r2_train))\n",
        "\n",
        "        # TODO: Test model on validation set using mean squared error and r-squared\n",
        "        predictions_val = None\n",
        "\n",
        "        score_mse_val = 0.0\n",
        "        print('Validation set mean squared error: {:.4f}'.format(score_mse_val))\n",
        "\n",
        "        score_r2_val = 0.0\n",
        "        print('Validation set r-squared scores: {:.4f}'.format(score_r2_val))\n",
        "\n",
        "        # TODO: Test model on testing set using mean squared error and r-squared\n",
        "        predictions_test = None\n",
        "\n",
        "        score_mse_test = 0.0\n",
        "        print('Testing set mean squared error: {:.4f}'.format(score_mse_test))\n",
        "\n",
        "        score_r2_test = 0.0\n",
        "        print('Testing set r-squared scores: {:.4f}'.format(score_r2_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
